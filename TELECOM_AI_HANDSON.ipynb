{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "language_info": {
   "name": "sql"
  },
  "lastEditStatus": {
   "notebookId": "4t4soww6yy5sledq2kcm",
   "authorId": "3933357949396",
   "authorName": "TTAKAHASHI",
   "authorEmail": "tatsuya.takahashi@snowflake.com",
   "sessionId": "c42632f2-6b79-4aa2-9274-f12545e53c57",
   "lastEditTime": 1772085196397
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# 通信会社 ネットワーク品質 AI エージェント ハンズオン\n\nこのハンズオンでは、Snowflake の AI 機能を使って**エリアマスタ、通信品質データ、設備キャパシティデータ、気象データ**の整備から **「自然言語で質問できる AI エージェント作成」** までを一気通貫で構築します。\n\n### 今日つくるもの\n\n携帯電話通信会社のネットワーク品質データ（通信速度・遅延・接続成功率・トラフィック量など）を横断的に分析できる AI エージェントです。  \n完成すると、例えばこんな質問に自然言語で答えてくれます：\n\n> 「2025年の東京エリアの通信速度推移を月別で可視化して。気温との相関も見たい」  \n> 「現在設備が逼迫しているエリアと、そのトラフィック推移を教えて」  \n> 「関東地方のエリア一覧とそれぞれの特徴を教えて」  \n> 「雨の日と晴れの日で通信品質にどんな違いがある？」\n\n---\n\n### ハンズオンの流れ（所要時間：約80分 ＋ 応用25分）\n\n| Step | やること | 学べる Snowflake 機能 | 所要時間 |\n|:----:|----------|----------------------|:--------:|\n| 1 | AI でエリアマスタを整備 | AI_CLASSIFY / AI_COMPLETE | 10分 |\n| 2 | 気象データを取り込む | Snowflake Marketplace | 10分 |\n| 3 | 通信品質の予兆検知（ML） | ML Functions（ANOMALY_DETECTION / FORECAST / TOP_INSIGHTS） | 20分 |\n| 4 | エリアの意味検索を可能にする | Cortex Search Service | 10分 |\n| 5 | データの意味を定義する | Semantic View（Cortex Analyst） | 20分 |\n| 6 | AI エージェントを組み立てる | Cortex Agent | 15分 |\n| 7 | 動かしてみよう！ | Cortex Agent UI | 5分 |\n| 8 | 【応用】カスタムモデルで予兆検知 | Snowpark ML / Container Runtime / Model Registry | 25分 |\n\n> **Note**: Step 8 は応用編です。時間に余裕がある場合にチャレンジしてください。Step 1〜7 だけでも十分にハンズオンは完結します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## Step 0: 事前準備\n",
    "使用するデータベース・スキーマ・ウェアハウスを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "name": "cell3",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "USE DATABASE TELECOM_AI_HANDSON;\n",
    "USE SCHEMA ANALYTICS;\n",
    "USE WAREHOUSE COMPUTE_WH;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "---\n",
    "## Step 1: AI でエリアマスタを整備する\n",
    "\n",
    "現在のエリアマスタ（`RAW.AREA_MASTER`）には、エリアID・エリア名・地方名しかありません。  \n",
    "ここに Snowflake の **AI SQL 関数** を使って、2つの情報を自動で追加します。\n",
    "\n",
    "### 追加する情報\n",
    "\n",
    "#### 1. エリアタイプの自動分類 — `AI_CLASSIFY`\n",
    "\n",
    "エリア名だけを見て、AI が自動的にエリアタイプを判定します。  \n",
    "あらかじめ定義した **5つのタイプ** に分類します（MECE＝漏れなくダブりなく）。\n",
    "\n",
    "| エリアタイプ | どんなエリア？ |\n",
    "|-------------|---------------|\n",
    "| 都心ビジネス | オフィス街、駅前商業地区、高層ビル密集地 |\n",
    "| 商業・繁華街 | ショッピングモール、繁華街、娯楽施設集中地 |\n",
    "| 住宅地 | マンション・戸建て住宅が中心の生活エリア |\n",
    "| 観光・ウォーターフロント | 観光地、海辺、リゾート、歴史的エリア |\n",
    "| 郊外・地方 | 工業地帯、農村部、山間地、人口密度が低いエリア |\n",
    "\n",
    "#### 2. エリア特性説明文の自動生成 — `AI_COMPLETE`\n",
    "\n",
    "エリア名と地方名から、そのエリアの **通信需要の特性や課題を説明する文章（約300文字）** を AI が自動で生成します。\n",
    "\n",
    "> **ポイント**: `AI_CLASSIFY` はラベル＋説明文を渡すだけで分類してくれるので、従来のルールベースのロジックを書く必要がありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "name": "cell5",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "SELECT * FROM TELECOM_AI_HANDSON.RAW.AREA_MASTER;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "それでは実行しましょう。エリア数にもよりますが、**約1〜2分** で完了します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "name": "cell7",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE TELECOM_AI_HANDSON.ANALYTICS.AI_AREA_MASTER AS\n",
    "SELECT \n",
    "    AREA_ID,\n",
    "    AREA_NAME,\n",
    "    REGION_NAME,\n",
    "    REGION_CODE,\n",
    "    -- ① AI_CLASSIFY関数でエリアタイプを分類（MECE）\n",
    "    AI_CLASSIFY(\n",
    "        AREA_NAME,\n",
    "        [\n",
    "            {'label': '都心ビジネス', 'description': 'オフィス街、駅前商業地区、高層ビル密集地、ビジネス中心地'},\n",
    "            {'label': '商業・繁華街', 'description': 'ショッピングモール、繁華街、娯楽施設集中地、歓楽街'},\n",
    "            {'label': '住宅地', 'description': 'マンション・戸建て住宅が中心の生活エリア、ベッドタウン'},\n",
    "            {'label': '観光・ウォーターフロント', 'description': '観光地、海辺、リゾート、歴史的エリア、臨海地区、湾岸'},\n",
    "            {'label': '郊外・地方', 'description': '工業地帯、農村部、山間地、人口密度が低いエリア、地方都市郊外'}\n",
    "        ],\n",
    "        {'task_description': '日本の携帯電話通信エリア名から最も適切なエリアタイプに分類してください'}\n",
    "    ):labels[0]::VARCHAR AS AREA_TYPE,\n",
    "    -- ② AI_COMPLETE関数でエリア特性説明文生成\n",
    "    TRIM(SNOWFLAKE.CORTEX.AI_COMPLETE(\n",
    "        'llama3.1-405b',\n",
    "        '以下の携帯電話通信エリアについて、通信需要の特性や課題を説明する文章（300文字程度）を日本語で1つ作成してください。\n",
    "         エリア名: ' || AREA_NAME || '\n",
    "         地方: ' || COALESCE(REGION_NAME, '不明') || '\n",
    "         説明文:'\n",
    "    )) AS AREA_DESCRIPTION,\n",
    "    CURRENT_TIMESTAMP() AS CREATED_AT\n",
    "FROM TELECOM_AI_HANDSON.RAW.AREA_MASTER;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "### 結果を見てみましょう\n",
    "\n",
    "AI が生成した `AREA_TYPE`（エリアタイプ）と `AREA_DESCRIPTION`（エリア特性説明文）を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "name": "cell9",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "SELECT * FROM TELECOM_AI_HANDSON.ANALYTICS.AI_AREA_MASTER;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "### エリアタイプ分類の精度を確認\n",
    "\n",
    "5つのエリアタイプにバランスよく分類されているか確認しましょう。  \n",
    "偏りがある場合は、`AI_CLASSIFY` に渡すエリアタイプの `description` を調整することで精度を改善できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "name": "cell11",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "    AREA_TYPE,\n",
    "    COUNT(*) AS AREA_COUNT\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.AI_AREA_MASTER\n",
    "GROUP BY AREA_TYPE\n",
    "ORDER BY AREA_COUNT DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "---\n",
    "## Step 2: 気象データを取り込む（Snowflake Marketplace）\n",
    "\n",
    "通信品質は天候に大きく左右されます。  \n",
    "雨天時は電波の減衰が増し通信品質が低下、台風時はさらに顕著 — こうした **天候×通信品質の分析** を可能にするため、  \n",
    "Snowflake Marketplace の **気象データ** を取り込みます。\n",
    "\n",
    "### データソース\n",
    "\n",
    "**Weather Source LLC: Frostbyte** が提供するグローバル気象データを使用します。  \n",
    "日本（JP）の日次気象データを 2019年〜 取得できます。\n",
    "\n",
    "### なぜテーブルを作るのか？\n",
    "\n",
    "元データは華氏・インチ単位で、グローバルデータが含まれています。  \n",
    "通信品質データと結合するには **日本のデータのみを抽出し、メートル法に変換して日付ごとに1行** にする必要があるため、  \n",
    "全国平均に集約するテーブルを作成します。\n",
    "\n",
    "| 集約カラム | 計算方法 | 説明 |\n",
    "|------------|----------|------|\n",
    "| 平均気温 | AVG | 全国の観測所の平均（℃に変換） |\n",
    "| 最高気温 | MAX | 全国で最も高かった気温（℃に変換） |\n",
    "| 最低気温 | MIN | 全国で最も低かった気温（℃に変換） |\n",
    "| 降水量 | AVG | 全国平均の降水量（mmに変換） |\n",
    "| 湿度 | AVG | 全国平均の湿度 |\n",
    "| 日照時間 | AVG | 全国平均の日射量 |\n",
    "| 降雪量 | SUM | 全国の降雪量合計（cmに変換） |\n",
    "| 風速 | AVG | 全国平均の風速（km/hに変換） |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "name": "cell13",
    "language": "sql"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE TELECOM_AI_HANDSON.ANALYTICS.DAILY_WEATHER AS\nSELECT\n    DATE_VALID_STD                                                      AS WEATHER_DATE,\n    ROUND(AVG((AVG_TEMPERATURE_AIR_2M_F - 32) * 5/9), 1)                AS AVG_TEMPERATURE,\n    ROUND(MAX((MAX_TEMPERATURE_AIR_2M_F - 32) * 5/9), 1)                AS MAX_TEMPERATURE,\n    ROUND(MIN((MIN_TEMPERATURE_AIR_2M_F - 32) * 5/9), 1)                AS MIN_TEMPERATURE,\n    ROUND(AVG(TOT_PRECIPITATION_IN * 25.4), 1)                          AS AVG_RAINFALL,\n    ROUND(AVG(AVG_HUMIDITY_RELATIVE_2M_PCT), 1)                         AS AVG_HUMIDITY,\n    ROUND(AVG(AVG_RADIATION_SOLAR_TOTAL_WPM2), 2)                       AS AVG_SUNLIGHT_HOURS,\n    ROUND(SUM(TOT_SNOWFALL_IN * 25.4), 1)                               AS TOTAL_SNOWFALL,\n    ROUND(AVG(AVG_WIND_SPEED_10M_MPH * 1.60934), 1)                     AS AVG_WIND_SPEED\nFROM WEATHER_SOURCE_LLC_FROSTBYTE.ONPOINT_ID.HISTORY_DAY\nWHERE COUNTRY = 'JP'\nGROUP BY DATE_VALID_STD\nORDER BY DATE_VALID_STD;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eaa4be-32fe-420e-8f8d-a95ba0ec9f37",
   "metadata": {
    "name": "cell14",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "SELECT * FROM TELECOM_AI_HANDSON.ANALYTICS.DAILY_WEATHER;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92308d14-64c2-4baf-9857-a07a4028afb8",
   "metadata": {
    "name": "cell26"
   },
   "source": [
    "---\n",
    "## Step 3: 通信品質の予兆検知（ML Functions）\n",
    "\n",
    "Step 2 で取り込んだ気象データと通信品質データを組み合わせて、**SQL だけで機械学習の予兆検知パイプライン** を構築します。\n",
    "\n",
    "### この Step で学ぶこと\n",
    "\n",
    "| パート | やること | 使う ML Function |\n",
    "|:------:|----------|:----------------:|\n",
    "| A | 切断率の異常を検知する | `ANOMALY_DETECTION` |\n",
    "| B | トラフィック量の将来予測 | `FORECAST` |\n",
    "| C | 品質劣化の要因を特定する | `TOP_INSIGHTS` |\n",
    "\n",
    "### ML Functions とは？\n",
    "\n",
    "Snowflake の **ML Functions** は、SQL だけで機械学習モデルの訓練・推論ができる機能です。  \n",
    "Python の知識は不要で、`CREATE SNOWFLAKE.ML.ANOMALY_DETECTION` のように SQL 文でモデルを作成できます。  \n",
    "内部的には Gradient Boosting Machine（GBM）や ARIMA 等のアルゴリズムが使われています。\n",
    "\n",
    "> **ポイント**: ML Functions は Standard Edition 以上の全エディションで利用可能です。"
   ],
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "id": "e3ce73d2-f746-47ac-832e-817470278f10",
   "metadata": {
    "name": "cell27"
   },
   "source": [
    "### パート A: 切断率の異常検知（ANOMALY_DETECTION）\n",
    "\n",
    "通信品質データの中でも **切断率（CALL_DROP_RATE）** は、値が跳ね上がるとユーザー体験に直結する重要指標です。  \n",
    "ここでは、全国平均の日次切断率を使って異常検知モデルを訓練し、品質劣化イベントを自動検出します。\n",
    "\n",
    "#### ステップ 1: 訓練データとテストデータを準備\n",
    "\n",
    "- **訓練データ**: 2022年〜2024年（3年分の通常パターンを学習）\n",
    "- **テストデータ**: 2025年〜（異常を検知する対象期間）"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "35e36f0a-39a8-463d-8a9b-64740bbf8145",
   "metadata": {
    "name": "cell28",
    "language": "sql"
   },
   "source": [
    "-- 訓練データ: 全国平均の日次切断率（2022〜2024年）\n",
    "CREATE OR REPLACE VIEW TELECOM_AI_HANDSON.ANALYTICS.NQ_TRAIN_DATA AS\n",
    "SELECT\n",
    "    MEASURE_DATE::TIMESTAMP_NTZ AS TS,\n",
    "    AVG(CALL_DROP_RATE)::FLOAT AS CALL_DROP_RATE\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.NETWORK_QUALITY\n",
    "WHERE MEASURE_DATE BETWEEN '2022-01-01' AND '2024-12-31'\n",
    "GROUP BY MEASURE_DATE\n",
    "ORDER BY MEASURE_DATE;\n",
    "\n",
    "-- テストデータ: 2025年以降\n",
    "CREATE OR REPLACE VIEW TELECOM_AI_HANDSON.ANALYTICS.NQ_TEST_DATA AS\n",
    "SELECT\n",
    "    MEASURE_DATE::TIMESTAMP_NTZ AS TS,\n",
    "    AVG(CALL_DROP_RATE)::FLOAT AS CALL_DROP_RATE\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.NETWORK_QUALITY\n",
    "WHERE MEASURE_DATE >= '2025-01-01'\n",
    "GROUP BY MEASURE_DATE\n",
    "ORDER BY MEASURE_DATE;\n",
    "\n",
    "-- 確認\n",
    "SELECT 'TRAIN' AS DATASET, COUNT(*) AS DAYS, MIN(TS) AS START_DATE, MAX(TS) AS END_DATE FROM TELECOM_AI_HANDSON.ANALYTICS.NQ_TRAIN_DATA\n",
    "UNION ALL\n",
    "SELECT 'TEST', COUNT(*), MIN(TS), MAX(TS) FROM TELECOM_AI_HANDSON.ANALYTICS.NQ_TEST_DATA;"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e099a016-b186-4aca-a6af-984b2059fe8c",
   "metadata": {
    "name": "cell29"
   },
   "source": [
    "#### ステップ 2: 基本の異常検知モデルを訓練\n",
    "\n",
    "まずは外因変数なしの **教師なし（Unsupervised）** モデルを作成します。  \n",
    "切断率の時系列パターンだけを学習し、そこから外れるデータを「異常」と判定します。"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "cdc069b8-f499-4787-b19d-3df43c8a01b8",
   "metadata": {
    "name": "cell30",
    "language": "sql"
   },
   "source": [
    "-- 基本モデルの訓練（教師なし、外因変数なし）\n",
    "CREATE OR REPLACE SNOWFLAKE.ML.ANOMALY_DETECTION TELECOM_AI_HANDSON.ANALYTICS.NQ_ANOMALY_BASE(\n",
    "    INPUT_DATA => TABLE(TELECOM_AI_HANDSON.ANALYTICS.NQ_TRAIN_DATA),\n",
    "    TIMESTAMP_COLNAME => 'TS',\n",
    "    TARGET_COLNAME => 'CALL_DROP_RATE',\n",
    "    LABEL_COLNAME => ''\n",
    ");"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f2f45754-72e5-44d6-99a9-8501e57ca8a9",
   "metadata": {
    "name": "cell31",
    "language": "sql"
   },
   "source": [
    "-- テストデータに対して異常を検知\n",
    "CREATE OR REPLACE TABLE TELECOM_AI_HANDSON.ANALYTICS.ANOMALY_RESULTS AS\n",
    "SELECT * FROM TABLE(TELECOM_AI_HANDSON.ANALYTICS.NQ_ANOMALY_BASE!DETECT_ANOMALIES(\n",
    "    INPUT_DATA => TABLE(TELECOM_AI_HANDSON.ANALYTICS.NQ_TEST_DATA),\n",
    "    TIMESTAMP_COLNAME => 'TS',\n",
    "    TARGET_COLNAME => 'CALL_DROP_RATE',\n",
    "    CONFIG_OBJECT => {'prediction_interval': 0.95}\n",
    "));\n",
    "\n",
    "-- 検知された異常日を確認\n",
    "SELECT\n",
    "    TS::DATE AS ANOMALY_DATE,\n",
    "    ROUND(Y, 4) AS ACTUAL_DROP_RATE,\n",
    "    ROUND(FORECAST, 4) AS EXPECTED_DROP_RATE,\n",
    "    ROUND(UPPER_BOUND, 4) AS UPPER_BOUND,\n",
    "    ROUND(Y - UPPER_BOUND, 4) AS EXCESS\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.ANOMALY_RESULTS\n",
    "WHERE IS_ANOMALY = TRUE\n",
    "ORDER BY TS;"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e6d9af39-9bc7-4590-80ee-f29ddda84d3d",
   "metadata": {
    "name": "cell32"
   },
   "source": [
    "#### ステップ 3: カスタム — 気象データを外因変数として追加\n",
    "\n",
    "基本モデルは切断率の時系列パターンのみで判断しますが、**降水量や風速が高い日は電波の減衰が増し、切断率が上がりやすい** ことがわかっています。  \n",
    "気象データを **外因変数（Exogenous Variables）** としてモデルに追加し、天候を考慮した異常検知に改善しましょう。\n",
    "\n",
    "> **外因変数とは？**: モデルが予測に使う「追加ヒント」です。タイムスタンプとターゲット以外のカラムは、すべて自動的に外因変数として扱われます。"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "1c51ce4c-0417-481d-8d88-64bbfe81e9e7",
   "metadata": {
    "name": "cell33",
    "language": "sql"
   },
   "source": [
    "-- 気象データ付き訓練データ（降水量・風速を外因変数として追加）\n",
    "CREATE OR REPLACE VIEW TELECOM_AI_HANDSON.ANALYTICS.NQ_TRAIN_WITH_WEATHER AS\n",
    "SELECT\n",
    "    nq.MEASURE_DATE::TIMESTAMP_NTZ AS TS,\n",
    "    AVG(nq.CALL_DROP_RATE)::FLOAT AS CALL_DROP_RATE,\n",
    "    MAX(w.AVG_RAINFALL)::FLOAT AS AVG_RAINFALL,\n",
    "    MAX(w.AVG_WIND_SPEED)::FLOAT AS AVG_WIND_SPEED\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.NETWORK_QUALITY nq\n",
    "JOIN TELECOM_AI_HANDSON.ANALYTICS.DAILY_WEATHER w\n",
    "    ON nq.MEASURE_DATE = w.WEATHER_DATE\n",
    "WHERE nq.MEASURE_DATE BETWEEN '2022-01-01' AND '2024-12-31'\n",
    "GROUP BY nq.MEASURE_DATE\n",
    "ORDER BY nq.MEASURE_DATE;\n",
    "\n",
    "-- 気象データ付きテストデータ\n",
    "CREATE OR REPLACE VIEW TELECOM_AI_HANDSON.ANALYTICS.NQ_TEST_WITH_WEATHER AS\n",
    "SELECT\n",
    "    nq.MEASURE_DATE::TIMESTAMP_NTZ AS TS,\n",
    "    AVG(nq.CALL_DROP_RATE)::FLOAT AS CALL_DROP_RATE,\n",
    "    MAX(w.AVG_RAINFALL)::FLOAT AS AVG_RAINFALL,\n",
    "    MAX(w.AVG_WIND_SPEED)::FLOAT AS AVG_WIND_SPEED\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.NETWORK_QUALITY nq\n",
    "JOIN TELECOM_AI_HANDSON.ANALYTICS.DAILY_WEATHER w\n",
    "    ON nq.MEASURE_DATE = w.WEATHER_DATE\n",
    "WHERE nq.MEASURE_DATE >= '2025-01-01'\n",
    "GROUP BY nq.MEASURE_DATE\n",
    "ORDER BY nq.MEASURE_DATE;\n",
    "\n",
    "-- 気象データ付きモデルの訓練\n",
    "CREATE OR REPLACE SNOWFLAKE.ML.ANOMALY_DETECTION TELECOM_AI_HANDSON.ANALYTICS.NQ_ANOMALY_WEATHER(\n",
    "    INPUT_DATA => TABLE(TELECOM_AI_HANDSON.ANALYTICS.NQ_TRAIN_WITH_WEATHER),\n",
    "    TIMESTAMP_COLNAME => 'TS',\n",
    "    TARGET_COLNAME => 'CALL_DROP_RATE',\n",
    "    LABEL_COLNAME => ''\n",
    ");"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6e8afdab-efbd-4c2c-a1f8-d786bcddc3d5",
   "metadata": {
    "name": "cell34"
   },
   "source": [
    "#### ステップ 4: 特徴量の重要度を確認\n",
    "\n",
    "`!EXPLAIN_FEATURE_IMPORTANCE()` で、モデルが異常検知にどの特徴量をどれだけ使っているかを確認します。  \n",
    "降水量や風速が上位に来ていれば、天候が切断率に影響していることをモデルが学習できています。"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "d9436d02-b694-4372-a4df-34195e0366d2",
   "metadata": {
    "name": "cell35",
    "language": "sql"
   },
   "source": [
    "-- 特徴量の重要度を確認\n",
    "CALL TELECOM_AI_HANDSON.ANALYTICS.NQ_ANOMALY_WEATHER!EXPLAIN_FEATURE_IMPORTANCE();"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c3e9e753-b001-468d-b25d-afb660103ce8",
   "metadata": {
    "name": "cell36"
   },
   "source": [
    "#### ステップ 5: カスタム — prediction_interval を変更して感度を比較\n",
    "\n",
    "`prediction_interval` は「どの程度の信頼区間で異常と判定するか」を制御するパラメータです。\n",
    "\n",
    "- **0.95**（デフォルト）: 95% 信頼区間 → やや敏感（異常を多く検知）\n",
    "- **0.99**: 99% 信頼区間 → より厳格（明らかな異常だけ検知）\n",
    "\n",
    "両方の結果を比較して、**感度 vs 特異度のトレードオフ** を体験しましょう。"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "5f72fd60-62ba-47f7-87c5-e9695dd29592",
   "metadata": {
    "name": "cell37",
    "language": "sql"
   },
   "source": [
    "-- 気象データ付きモデルで異常検知（prediction_interval = 0.95）\n",
    "CREATE OR REPLACE TABLE TELECOM_AI_HANDSON.ANALYTICS.ANOMALY_WEATHER_95 AS\n",
    "SELECT * FROM TABLE(TELECOM_AI_HANDSON.ANALYTICS.NQ_ANOMALY_WEATHER!DETECT_ANOMALIES(\n",
    "    INPUT_DATA => TABLE(TELECOM_AI_HANDSON.ANALYTICS.NQ_TEST_WITH_WEATHER),\n",
    "    TIMESTAMP_COLNAME => 'TS',\n",
    "    TARGET_COLNAME => 'CALL_DROP_RATE',\n",
    "    CONFIG_OBJECT => {'prediction_interval': 0.95}\n",
    "));\n",
    "\n",
    "-- 同じモデルで prediction_interval = 0.99 で再検知\n",
    "CREATE OR REPLACE TABLE TELECOM_AI_HANDSON.ANALYTICS.ANOMALY_WEATHER_99 AS\n",
    "SELECT * FROM TABLE(TELECOM_AI_HANDSON.ANALYTICS.NQ_ANOMALY_WEATHER!DETECT_ANOMALIES(\n",
    "    INPUT_DATA => TABLE(TELECOM_AI_HANDSON.ANALYTICS.NQ_TEST_WITH_WEATHER),\n",
    "    TIMESTAMP_COLNAME => 'TS',\n",
    "    TARGET_COLNAME => 'CALL_DROP_RATE',\n",
    "    CONFIG_OBJECT => {'prediction_interval': 0.99}\n",
    "));\n",
    "\n",
    "-- 感度比較: 各 interval で検出された異常数を比較\n",
    "SELECT\n",
    "    '0.95 (デフォルト)' AS PREDICTION_INTERVAL,\n",
    "    COUNT(*) AS TOTAL_DAYS,\n",
    "    SUM(CASE WHEN IS_ANOMALY THEN 1 ELSE 0 END) AS ANOMALY_DAYS,\n",
    "    ROUND(SUM(CASE WHEN IS_ANOMALY THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) AS ANOMALY_PCT\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.ANOMALY_WEATHER_95\n",
    "UNION ALL\n",
    "SELECT\n",
    "    '0.99 (厳格)',\n",
    "    COUNT(*),\n",
    "    SUM(CASE WHEN IS_ANOMALY THEN 1 ELSE 0 END),\n",
    "    ROUND(SUM(CASE WHEN IS_ANOMALY THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1)\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.ANOMALY_WEATHER_99;"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8eb1f6bd-82c6-4fed-ac38-736f954d6ff4",
   "metadata": {
    "name": "cell38"
   },
   "source": [
    "### パート B: トラフィック量の将来予測（FORECAST）\n",
    "\n",
    "設備が **逼迫** しているエリアのトラフィック量を予測し、**あと何日で設備容量を超過するか** をシミュレーションします。  \n",
    "ここでも気象データを外因変数として追加し、予測精度の改善を体験します。\n",
    "\n",
    "#### ステップ 1: 設備逼迫エリアの確認とトレーニングデータ準備"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "bd409819-ea86-4fd0-8a53-ce46eb2f9946",
   "metadata": {
    "name": "cell39",
    "language": "sql"
   },
   "source": [
    "-- 設備逼迫エリアを確認\n",
    "SELECT AREA_ID, AREA_NAME, REGION_NAME, CURRENT_LOAD_PCT, MAX_CAPACITY_GBPS, EQUIPMENT_STATUS\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.EQUIPMENT_STATUS\n",
    "WHERE EQUIPMENT_STATUS = '逼迫'\n",
    "ORDER BY CURRENT_LOAD_PCT DESC;"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cf276ccb-af05-4cdf-b5b2-d1ed31c2724b",
   "metadata": {
    "name": "cell40"
   },
   "source": [
    "#### ステップ 2: 気温を外因変数にしたトラフィック予測モデルの訓練\n",
    "\n",
    "東京都心南部エリア（負荷率92.2%で最も逼迫）を対象に、気温を外因変数として FORECAST モデルを訓練します。  \n",
    "夏場の暑い日はエアコン下での動画視聴が増え、冬場はイベント時のトラフィックが集中するなど、**気温とトラフィックには相関** があります。"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "007baeae-2ecb-4eae-8fcc-f8ef5a5831ff",
   "metadata": {
    "name": "cell41",
    "language": "sql"
   },
   "source": [
    "-- 訓練データ: 東京都心南部の日次トラフィック + 気温（2022〜2025年）\n",
    "CREATE OR REPLACE VIEW TELECOM_AI_HANDSON.ANALYTICS.TRAFFIC_TRAIN AS\n",
    "SELECT\n",
    "    nq.MEASURE_DATE::TIMESTAMP_NTZ AS TS,\n",
    "    nq.TOTAL_TRAFFIC_GB::FLOAT AS TOTAL_TRAFFIC_GB,\n",
    "    w.AVG_TEMPERATURE::FLOAT AS AVG_TEMPERATURE\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.NETWORK_QUALITY nq\n",
    "JOIN TELECOM_AI_HANDSON.ANALYTICS.DAILY_WEATHER w\n",
    "    ON nq.MEASURE_DATE = w.WEATHER_DATE\n",
    "WHERE nq.AREA_ID = 'AREA_KT_002'  -- 東京都心南部エリア\n",
    "ORDER BY nq.MEASURE_DATE;\n",
    "\n",
    "-- FORECAST モデルの訓練（気温を外因変数として使用）\n",
    "CREATE OR REPLACE SNOWFLAKE.ML.FORECAST TELECOM_AI_HANDSON.ANALYTICS.TRAFFIC_FORECAST_MODEL(\n",
    "    INPUT_DATA => TABLE(TELECOM_AI_HANDSON.ANALYTICS.TRAFFIC_TRAIN),\n",
    "    TIMESTAMP_COLNAME => 'TS',\n",
    "    TARGET_COLNAME => 'TOTAL_TRAFFIC_GB'\n",
    ");\n",
    "\n",
    "-- 特徴量の重要度を確認（気温がどれだけ寄与しているか）\n",
    "CALL TELECOM_AI_HANDSON.ANALYTICS.TRAFFIC_FORECAST_MODEL!EXPLAIN_FEATURE_IMPORTANCE();"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "06d266ad-c5c5-4f3b-9cb5-62078d280c46",
   "metadata": {
    "name": "cell42"
   },
   "source": [
    "#### ステップ 3: 30日先のトラフィック予測と設備容量超過シミュレーション\n",
    "\n",
    "外因変数（気温）を使ったモデルなので、予測期間の気温も渡す必要があります。  \n",
    "ここでは **前年同時期の気温データ** を将来の気温推定値として使う実践的テクニックを紹介します。"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "09411037-d9e1-4b55-ae8e-a06f8b12fe6b",
   "metadata": {
    "name": "cell43",
    "language": "sql"
   },
   "source": [
    "-- 将来30日分の気温データを前年同時期から生成\n",
    "-- FLOAT カラムを明示定義（FORECAST の外因変数は非NULL の FLOAT 型が必要）\n",
    "CREATE OR REPLACE TEMP TABLE TELECOM_AI_HANDSON.ANALYTICS.FUTURE_WEATHER (\n",
    "    TS TIMESTAMP_NTZ,\n",
    "    AVG_TEMPERATURE FLOAT\n",
    ");\n",
    "\n",
    "-- 前年同日の気温を取得。前年データがなければ前後1日で補完\n",
    "INSERT INTO TELECOM_AI_HANDSON.ANALYTICS.FUTURE_WEATHER\n",
    "WITH base AS (\n",
    "    SELECT DATEADD(DAY, 1, MAX(MEASURE_DATE)) AS START_DATE\n",
    "    FROM TELECOM_AI_HANDSON.ANALYTICS.NETWORK_QUALITY\n",
    "),\n",
    "future_dates AS (\n",
    "    SELECT\n",
    "        DATEADD(DAY, SEQ4(), (SELECT START_DATE FROM base))::TIMESTAMP_NTZ AS TS,\n",
    "        DATEADD(DAY, SEQ4(), (SELECT START_DATE FROM base))::DATE          AS FUTURE_DATE\n",
    "    FROM TABLE(GENERATOR(ROWCOUNT => 30))\n",
    ")\n",
    "SELECT\n",
    "    f.TS,\n",
    "    COALESCE(\n",
    "        w.AVG_TEMPERATURE,\n",
    "        -- 前年同日がなければ前年±1日の平均で補完\n",
    "        (SELECT AVG(w2.AVG_TEMPERATURE)\n",
    "         FROM TELECOM_AI_HANDSON.ANALYTICS.DAILY_WEATHER w2\n",
    "         WHERE w2.WEATHER_DATE BETWEEN DATEADD(YEAR, -1, DATEADD(DAY, -1, f.FUTURE_DATE))\n",
    "                                    AND DATEADD(YEAR, -1, DATEADD(DAY,  1, f.FUTURE_DATE)))\n",
    "    )::FLOAT AS AVG_TEMPERATURE\n",
    "FROM future_dates f\n",
    "LEFT JOIN TELECOM_AI_HANDSON.ANALYTICS.DAILY_WEATHER w\n",
    "    ON w.WEATHER_DATE = DATEADD(YEAR, -1, f.FUTURE_DATE)\n",
    "ORDER BY f.TS;\n",
    "\n",
    "-- NULL が残っていないことを確認\n",
    "SELECT COUNT(*) AS TOTAL, COUNT(AVG_TEMPERATURE) AS NON_NULL\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.FUTURE_WEATHER;\n",
    "\n",
    "-- 30日先のトラフィック予測を実行\n",
    "CREATE OR REPLACE TABLE TELECOM_AI_HANDSON.ANALYTICS.TRAFFIC_FORECAST_RESULTS AS\n",
    "SELECT * FROM TABLE(TELECOM_AI_HANDSON.ANALYTICS.TRAFFIC_FORECAST_MODEL!FORECAST(\n",
    "    INPUT_DATA => TABLE(TELECOM_AI_HANDSON.ANALYTICS.FUTURE_WEATHER),\n",
    "    TIMESTAMP_COLNAME => 'TS'\n",
    "));"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "568de5f6-099a-434c-9955-3c5ac5a49ad6",
   "metadata": {
    "name": "cell44",
    "language": "sql"
   },
   "source": [
    "-- 予測結果と設備容量を比較: いつ容量超過するか？\n",
    "-- 東京都心南部エリアの最大容量: 140.1 Gbps → 日次換算 (概算)\n",
    "WITH capacity AS (\n",
    "    SELECT MAX_CAPACITY_GBPS * 1000 AS DAILY_CAPACITY_GB  -- Gbps を概算で日次GBに換算\n",
    "    FROM TELECOM_AI_HANDSON.ANALYTICS.EQUIPMENT_STATUS\n",
    "    WHERE AREA_ID = 'AREA_KT_002'\n",
    ")\n",
    "SELECT\n",
    "    f.TS::DATE AS FORECAST_DATE,\n",
    "    ROUND(f.FORECAST, 1) AS PREDICTED_TRAFFIC_GB,\n",
    "    ROUND(f.UPPER_BOUND, 1) AS UPPER_BOUND_GB,\n",
    "    ROUND(c.DAILY_CAPACITY_GB, 1) AS CAPACITY_GB,\n",
    "    ROUND(f.FORECAST / c.DAILY_CAPACITY_GB * 100, 1) AS LOAD_PCT,\n",
    "    CASE\n",
    "        WHEN f.UPPER_BOUND > c.DAILY_CAPACITY_GB THEN '⚠ 超過リスクあり'\n",
    "        WHEN f.FORECAST > c.DAILY_CAPACITY_GB * 0.9 THEN '△ 注意'\n",
    "        ELSE '○ 正常'\n",
    "    END AS STATUS\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.TRAFFIC_FORECAST_RESULTS f\n",
    "CROSS JOIN capacity c\n",
    "ORDER BY f.TS;"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "77ddd874-264d-4489-96d7-48af7b058145",
   "metadata": {
    "name": "cell45"
   },
   "source": [
    "### パート C: 品質劣化の要因分析（TOP_INSIGHTS）\n",
    "\n",
    "パート A で切断率の異常日が特定できました。  \n",
    "次は **「なぜその日に品質が劣化したのか？」** を自動分析します。\n",
    "\n",
    "`TOP_INSIGHTS` は、**2つのグループ（異常期間 vs 通常期間）のメトリクスの差を分析し、どのディメンション（地方、エリアタイプなど）が最も差に寄与しているか** を決定木で自動特定します。"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "b1d6a14c-bfc7-49c6-8c79-5687711b04b5",
   "metadata": {
    "name": "cell46",
    "language": "sql"
   },
   "source": [
    "-- TOP_INSIGHTS で品質劣化の要因を分析\n",
    "-- 異常日 vs 通常日を label で区別し、地方・エリアごとの切断率差を自動特定\n",
    "CREATE OR REPLACE SNOWFLAKE.ML.TOP_INSIGHTS TELECOM_AI_HANDSON.ANALYTICS.NQ_INSIGHTS_MODEL();\n",
    "\n",
    "CREATE OR REPLACE VIEW TELECOM_AI_HANDSON.ANALYTICS.NQ_INSIGHTS_INPUT AS\n",
    "SELECT\n",
    "    nq.REGION_NAME,\n",
    "    nq.AREA_NAME,\n",
    "    nq.CALL_DROP_RATE::FLOAT AS CALL_DROP_RATE,\n",
    "    nq.PACKET_LOSS_RATE::FLOAT AS PACKET_LOSS_RATE,\n",
    "    w.AVG_RAINFALL::FLOAT AS AVG_RAINFALL,\n",
    "    w.AVG_WIND_SPEED::FLOAT AS AVG_WIND_SPEED,\n",
    "    -- 異常日かどうかのラベル（パートAの結果を利用）\n",
    "    CASE\n",
    "        WHEN nq.MEASURE_DATE IN (SELECT TS::DATE FROM TELECOM_AI_HANDSON.ANALYTICS.ANOMALY_WEATHER_95 WHERE IS_ANOMALY = TRUE)\n",
    "        THEN TRUE ELSE FALSE\n",
    "    END AS IS_ANOMALY_DAY\n",
    "FROM TELECOM_AI_HANDSON.ANALYTICS.NETWORK_QUALITY nq\n",
    "JOIN TELECOM_AI_HANDSON.ANALYTICS.DAILY_WEATHER w\n",
    "    ON nq.MEASURE_DATE = w.WEATHER_DATE\n",
    "WHERE nq.MEASURE_DATE >= '2025-01-01';\n",
    "\n",
    "CALL TELECOM_AI_HANDSON.ANALYTICS.NQ_INSIGHTS_MODEL!GET_DRIVERS(\n",
    "    INPUT_DATA => TABLE(TELECOM_AI_HANDSON.ANALYTICS.NQ_INSIGHTS_INPUT),\n",
    "    LABEL_COLNAME => 'IS_ANOMALY_DAY',\n",
    "    METRIC_COLNAME => 'CALL_DROP_RATE'\n",
    ");"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd6341e-1c3b-4547-bc33-3d9225d63123",
   "metadata": {
    "name": "cell47"
   },
   "source": [
    "### Step 3 まとめ\n\nSQL だけで以下の ML パイプラインを構築できました：\n\n| 機能 | ML Function | カスタム要素 |\n|------|------------|------------|\n| 異常検知 | `ANOMALY_DETECTION` | 気象データを外因変数に追加、prediction_interval で感度調整 |\n| 将来予測 | `FORECAST` | 気温を外因変数に追加、前年データで将来値を生成 |\n| 要因分析 | `TOP_INSIGHTS` | 異常日 vs 通常日で地方・気象要因の寄与度を自動特定 |\n\n> **発展**: この異常検知を Snowflake の **Task + Alert** と組み合わせると、「切断率が異常値を超えたら自動でメール通知」という運用パイプラインも構築できます。\n\n> **応用編**: Python で独自モデル（IsolationForest / XGBoost）を構築したい方は **Step 8** に進んでください。"
   ],
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "name": "cell15"
   },
   "source": [
    "---\n",
    "## Step 4: エリアの意味検索を可能にする（Cortex Search）\n",
    "\n",
    "Step 1 で作った `AI_AREA_MASTER` に対して、**Cortex Search Service** を作成します。\n",
    "\n",
    "### Cortex Search とは？\n",
    "\n",
    "テキストの **意味** を理解して検索するサービスです。  \n",
    "キーワードの完全一致ではなく、「こういう意味のものを探して」という検索ができます。\n",
    "\n",
    "例えば「トラフィックが多い都市部のエリア」と聞くと、東京都心や大阪梅田などのエリアがヒットします。\n",
    "\n",
    "### 仕組み\n",
    "\n",
    "エリア名・地方名・エリアタイプ・AI説明文を **1つのテキストに結合** し、  \n",
    "Snowflake の Embedding モデル（`snowflake-arctic-embed-l-v2.0`）でベクトル化して検索可能にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "name": "cell16",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE CORTEX SEARCH SERVICE TELECOM_AI_HANDSON.ANALYTICS.AREA_SEARCH_SERVICE\n",
    "    ON SEARCH_TEXT\n",
    "    ATTRIBUTES AREA_ID, AREA_NAME, REGION_NAME, AREA_TYPE, AREA_DESCRIPTION\n",
    "    WAREHOUSE = COMPUTE_WH\n",
    "    TARGET_LAG = '1 day'\n",
    "    EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n",
    "AS (\n",
    "    SELECT\n",
    "        AREA_ID,\n",
    "        AREA_NAME,\n",
    "        REGION_NAME,\n",
    "        REGION_CODE,\n",
    "        AREA_TYPE,\n",
    "        AREA_DESCRIPTION,\n",
    "        CONCAT(\n",
    "            COALESCE(AREA_NAME, ''), ' ',\n",
    "            COALESCE(REGION_NAME, ''), ' ',\n",
    "            COALESCE(AREA_TYPE, ''), ' ',\n",
    "            COALESCE(AREA_DESCRIPTION, '')\n",
    "        ) AS SEARCH_TEXT\n",
    "    FROM TELECOM_AI_HANDSON.ANALYTICS.AI_AREA_MASTER\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000017",
   "metadata": {
    "name": "cell17"
   },
   "source": [
    "---\n",
    "## Step 5: データの意味を定義する（セマンティックビュー）\n",
    "\n",
    "ここがこのハンズオンの **キモ** です。\n",
    "\n",
    "Cortex Analyst に「通信速度を教えて」と聞いたとき、どのテーブルの・どのカラムを・どう集計すればよいかを  \n",
    "AI が理解できるように、**データの意味（セマンティクス）** を定義します。\n",
    "\n",
    "### セマンティックビューで定義すること\n",
    "\n",
    "| 定義項目 | 何を定義する？ | 具体例 |\n",
    "|----------|---------------|--------|\n",
    "| **テーブル** | 分析対象のテーブルとその説明 | NETWORK_QUALITY, EQUIPMENT_STATUS, WEATHER |\n",
    "| **リレーションシップ** | テーブル間の結合条件 | 計測日 = 気象観測日 |\n",
    "| **ファクト** | 集計対象の数値カラム | 通信速度、遅延、トラフィック量、気温 |\n",
    "| **ディメンション** | 分析の切り口となる属性 | エリア名、地方、日付 |\n",
    "| **メトリクス** | よく使う集計パターン | 平均ダウンロード速度＝AVG(下り速度) |\n",
    "| **シノニム** | 日本語の別名・同義語 | 「通信速度」「下り速度」「ダウンロード速度」 |\n",
    "\n",
    "### 3つのテーブルの関係\n",
    "\n",
    "```\n",
    "              AREA_ID              MEASURE_DATE = WEATHER_DATE\n",
    "NETWORK_QUALITY ───────▶ EQUIPMENT_STATUS    NETWORK_QUALITY ────────────────▶ WEATHER\n",
    "(通信品質)                (設備状況)           (通信品質)                        (気象)\n",
    "```\n",
    "\n",
    "> **ポイント**: シノニムを豊富に定義しておくと、「通信速度」でも「ダウンロード速度」でも「下り」でも同じカラムを参照してくれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000018",
   "metadata": {
    "name": "cell18",
    "language": "sql"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE SEMANTIC VIEW TELECOM_AI_HANDSON.ANALYTICS.TELECOM_ANALYST\n\n    -- ========================================\n    -- テーブル定義\n    -- ========================================\n    tables (\n        NETWORK_QUALITY AS TELECOM_AI_HANDSON.ANALYTICS.NETWORK_QUALITY\n            PRIMARY KEY (AREA_ID, MEASURE_DATE)\n            WITH SYNONYMS = ('通信品質', 'ネットワーク品質', '品質データ', '通信データ', 'NW品質', '通信速度データ', 'KPI')\n            COMMENT = '日次ネットワーク品質データ。2022-01-01〜2026-02-24のエリア別日次通信速度・遅延・接続成功率・トラフィック量を含む。',\n\n        EQUIPMENT_STATUS AS TELECOM_AI_HANDSON.ANALYTICS.EQUIPMENT_STATUS\n            PRIMARY KEY (AREA_ID)\n            WITH SYNONYMS = ('設備', '設備状況', '設備データ', '基地局', 'キャパシティ', '設備キャパシティ', '負荷')\n            COMMENT = '設備キャパシティスナップショット（2026-02-24時点）。エリア別の基地局数・最大容量・負荷率・設備ステータスを含む。',\n\n        WEATHER AS TELECOM_AI_HANDSON.ANALYTICS.DAILY_WEATHER\n            PRIMARY KEY (WEATHER_DATE)\n            WITH SYNONYMS = ('気象', '天気', '気象データ', '天気データ', '気温', '降水量', '天候', '気象庁', '気象情報')\n            COMMENT = '全国日次平均気象データ（2015年〜）。気象庁の全観測所データを日別に集約した全国平均の気温・降水量・湿度・日照時間・降雪量・風速。'\n    )\n\n    -- ========================================\n    -- リレーションシップ定義\n    -- ========================================\n    relationships (\n        NQ_EQUIPMENT AS NETWORK_QUALITY(AREA_ID) REFERENCES EQUIPMENT_STATUS(AREA_ID),\n        NQ_WEATHER AS NETWORK_QUALITY(MEASURE_DATE) REFERENCES WEATHER(WEATHER_DATE)\n    )\n\n    -- ========================================\n    -- ファクト定義（数値データ）\n    -- ========================================\n    facts (\n        -- 通信品質ファクト\n        NETWORK_QUALITY.download_speed_fact AS AVG_DOWNLOAD_MBPS\n            COMMENT = '平均ダウンロード速度（Mbps）。下り通信速度。',\n        NETWORK_QUALITY.upload_speed_fact AS AVG_UPLOAD_MBPS\n            COMMENT = '平均アップロード速度（Mbps）。上り通信速度。',\n        NETWORK_QUALITY.latency_fact AS AVG_LATENCY_MS\n            COMMENT = '平均遅延（ミリ秒）。レイテンシ。',\n        NETWORK_QUALITY.connection_success_fact AS CONNECTION_SUCCESS_RATE\n            COMMENT = '接続成功率（%）。100%に近いほど良好。',\n        NETWORK_QUALITY.call_drop_fact AS CALL_DROP_RATE\n            COMMENT = '切断率（%）。0%に近いほど良好。',\n        NETWORK_QUALITY.packet_loss_fact AS PACKET_LOSS_RATE\n            COMMENT = 'パケットロス率（%）。0%に近いほど良好。',\n        NETWORK_QUALITY.traffic_fact AS TOTAL_TRAFFIC_GB\n            COMMENT = '日次総トラフィック量（GB）',\n\n        -- 設備ファクト\n        EQUIPMENT_STATUS.base_station_count_fact AS BASE_STATION_COUNT\n            COMMENT = '基地局数（局）',\n        EQUIPMENT_STATUS.max_capacity_fact AS MAX_CAPACITY_GBPS\n            COMMENT = '最大通信容量（Gbps）',\n        EQUIPMENT_STATUS.current_load_fact AS CURRENT_LOAD_PCT\n            COMMENT = '現在の設備負荷率（%）。100%に近いほど逼迫。',\n\n        -- 気象ファクト\n        WEATHER.avg_temperature_fact AS AVG_TEMPERATURE\n            COMMENT = '全国日平均気温（℃）。全観測所の平均。',\n        WEATHER.max_temperature_fact AS MAX_TEMPERATURE\n            COMMENT = '全国日最高気温（℃）。全観測所の最大値。',\n        WEATHER.min_temperature_fact AS MIN_TEMPERATURE\n            COMMENT = '全国日最低気温（℃）。全観測所の最小値。',\n        WEATHER.avg_rainfall_fact AS AVG_RAINFALL\n            COMMENT = '全国日平均降水量（mm）',\n        WEATHER.avg_humidity_fact AS AVG_HUMIDITY\n            COMMENT = '全国日平均湿度（%）',\n        WEATHER.avg_sunlight_hours_fact AS AVG_SUNLIGHT_HOURS\n            COMMENT = '全国日平均日照時間（時間）',\n        WEATHER.total_snowfall_fact AS TOTAL_SNOWFALL\n            COMMENT = '全国日降雪量合計（cm）',\n        WEATHER.avg_wind_speed_fact AS AVG_WIND_SPEED\n            COMMENT = '全国日平均風速（km/h）'\n    )\n\n    -- ========================================\n    -- ディメンション定義（属性データ）\n    -- ========================================\n    dimensions (\n        -- ========== NETWORK_QUALITY ディメンション ==========\n        NETWORK_QUALITY.area_id AS AREA_ID\n            WITH SYNONYMS = ('エリアID', 'エリアコード', '基地局エリア', 'エリア識別子')\n            COMMENT = 'エリア識別コード（例：AREA_KT_001）',\n        NETWORK_QUALITY.area_name AS AREA_NAME\n            WITH SYNONYMS = ('エリア名', 'エリア', '地域名', '地区名', '基地局エリア名')\n            COMMENT = '通信エリア名。東京都心北部エリア、大阪梅田エリアなどを含む。',\n        NETWORK_QUALITY.region_name AS REGION_NAME\n            WITH SYNONYMS = ('地方', '地方名', 'リージョン', '地域')\n            COMMENT = '地方名（関東、近畿、東海、九州）',\n        NETWORK_QUALITY.measure_date AS MEASURE_DATE\n            WITH SYNONYMS = ('計測日', '測定日', '日付', '対象日')\n            COMMENT = '通信品質を計測した日付（DATE型、2022-01-01〜2026-02-24）',\n        NETWORK_QUALITY.measure_year AS YEAR(MEASURE_DATE)\n            WITH SYNONYMS = ('計測年', '年', '対象年')\n            COMMENT = '計測年（2022〜2026）',\n        NETWORK_QUALITY.measure_month AS MONTH(MEASURE_DATE)\n            WITH SYNONYMS = ('計測月', '月', '対象月')\n            COMMENT = '計測月（1〜12）',\n\n        -- ========== EQUIPMENT_STATUS ディメンション ==========\n        EQUIPMENT_STATUS.area_id AS AREA_ID\n            WITH SYNONYMS = ('エリアID', 'エリアコード')\n            COMMENT = 'エリア識別コード',\n        EQUIPMENT_STATUS.area_name AS AREA_NAME\n            WITH SYNONYMS = ('エリア名', 'エリア')\n            COMMENT = 'エリア名',\n        EQUIPMENT_STATUS.region_name AS REGION_NAME\n            WITH SYNONYMS = ('地方', '地方名')\n            COMMENT = '地方名',\n        EQUIPMENT_STATUS.snapshot_date AS SNAPSHOT_DATE\n            WITH SYNONYMS = ('スナップショット日', '基準日', '設備確認日')\n            COMMENT = '設備データの取得日（2026-02-24）',\n        EQUIPMENT_STATUS.equipment_status AS EQUIPMENT_STATUS\n            WITH SYNONYMS = ('設備ステータス', '設備状態', '設備区分', 'ステータス', '負荷状態')\n            COMMENT = '設備の負荷状態を表すステータス（正常 / 注意 / 逼迫）',\n\n        -- ========== WEATHER ディメンション ==========\n        WEATHER.weather_date AS WEATHER_DATE\n            WITH SYNONYMS = ('観測日', '気象日', '天気の日付', '気象観測日')\n            COMMENT = '気象データの観測日（DATE型、2015年〜）。NETWORK_QUALITYのMEASURE_DATEとリレーションで紐づく。',\n        WEATHER.weather_year AS YEAR(WEATHER_DATE)\n            WITH SYNONYMS = ('観測年', '気象年')\n            COMMENT = '気象データの観測年',\n        WEATHER.weather_month AS MONTH(WEATHER_DATE)\n            WITH SYNONYMS = ('観測月', '気象月')\n            COMMENT = '気象データの観測月（1〜12）'\n    )\n\n    -- ========================================\n    -- メトリクス定義（集計関数）\n    -- ========================================\n    metrics (\n        -- 通信品質メトリクス\n        NETWORK_QUALITY.avg_download_speed AS AVG(NETWORK_QUALITY.download_speed_fact)\n            WITH SYNONYMS = ('平均ダウンロード速度', '平均下り速度', '下り平均', '平均通信速度')\n            COMMENT = 'ダウンロード速度の平均（Mbps）',\n        NETWORK_QUALITY.avg_upload_speed AS AVG(NETWORK_QUALITY.upload_speed_fact)\n            WITH SYNONYMS = ('平均アップロード速度', '平均上り速度', '上り平均')\n            COMMENT = 'アップロード速度の平均（Mbps）',\n        NETWORK_QUALITY.avg_latency AS AVG(NETWORK_QUALITY.latency_fact)\n            WITH SYNONYMS = ('平均遅延', '平均レイテンシ', 'レイテンシ平均')\n            COMMENT = '遅延の平均（ms）',\n        NETWORK_QUALITY.avg_connection_success AS AVG(NETWORK_QUALITY.connection_success_fact)\n            WITH SYNONYMS = ('平均接続成功率', '接続成功率平均', '接続率')\n            COMMENT = '接続成功率の平均（%）',\n        NETWORK_QUALITY.avg_call_drop AS AVG(NETWORK_QUALITY.call_drop_fact)\n            WITH SYNONYMS = ('平均切断率', '切断率平均', '通話切断率')\n            COMMENT = '切断率の平均（%）',\n        NETWORK_QUALITY.avg_packet_loss AS AVG(NETWORK_QUALITY.packet_loss_fact)\n            WITH SYNONYMS = ('平均パケットロス率', 'パケロス平均', 'ロス率')\n            COMMENT = 'パケットロス率の平均（%）',\n        NETWORK_QUALITY.total_traffic AS SUM(NETWORK_QUALITY.traffic_fact)\n            WITH SYNONYMS = ('総トラフィック', '合計トラフィック', 'トラフィック合計', '総通信量')\n            COMMENT = 'トラフィック量の合計（GB）',\n        NETWORK_QUALITY.avg_daily_traffic AS AVG(NETWORK_QUALITY.traffic_fact)\n            WITH SYNONYMS = ('日平均トラフィック', '日次平均トラフィック', '平均日次通信量')\n            COMMENT = '日次トラフィック量の平均（GB）',\n\n        -- 設備メトリクス\n        EQUIPMENT_STATUS.total_base_stations AS SUM(EQUIPMENT_STATUS.base_station_count_fact)\n            WITH SYNONYMS = ('総基地局数', '合計基地局数', '基地局数合計')\n            COMMENT = '基地局数の合計（局）',\n        EQUIPMENT_STATUS.total_capacity AS SUM(EQUIPMENT_STATUS.max_capacity_fact)\n            WITH SYNONYMS = ('総容量', '合計容量', '最大容量合計')\n            COMMENT = '最大通信容量の合計（Gbps）',\n        EQUIPMENT_STATUS.avg_load AS AVG(EQUIPMENT_STATUS.current_load_fact)\n            WITH SYNONYMS = ('平均負荷率', '負荷率平均', '平均設備負荷')\n            COMMENT = '設備負荷率の平均（%）',\n\n        -- 気象メトリクス\n        WEATHER.period_avg_temperature AS AVG(WEATHER.avg_temperature_fact)\n            WITH SYNONYMS = ('平均気温', '気温平均', '平均温度')\n            COMMENT = '期間内の全国平均気温（℃）',\n        WEATHER.period_max_temperature AS MAX(WEATHER.max_temperature_fact)\n            WITH SYNONYMS = ('最高気温', '最高温度', '期間最高気温')\n            COMMENT = '期間中の全国最高気温（℃）',\n        WEATHER.period_min_temperature AS MIN(WEATHER.min_temperature_fact)\n            WITH SYNONYMS = ('最低気温', '最低温度', '期間最低気温')\n            COMMENT = '期間中の全国最低気温（℃）',\n        WEATHER.period_avg_rainfall AS AVG(WEATHER.avg_rainfall_fact)\n            WITH SYNONYMS = ('平均降水量', '降水量平均')\n            COMMENT = '期間内の全国平均降水量（mm）',\n        WEATHER.period_avg_humidity AS AVG(WEATHER.avg_humidity_fact)\n            WITH SYNONYMS = ('平均湿度', '湿度平均')\n            COMMENT = '期間内の全国平均湿度（%）',\n        WEATHER.period_avg_sunlight AS AVG(WEATHER.avg_sunlight_hours_fact)\n            WITH SYNONYMS = ('平均日照時間', '日照時間平均')\n            COMMENT = '期間内の全国平均日照時間（時間）',\n        WEATHER.period_total_snowfall AS SUM(WEATHER.total_snowfall_fact)\n            WITH SYNONYMS = ('総降雪量', '合計降雪量', '累積降雪量')\n            COMMENT = '期間内の全国降雪量合計（cm）',\n        WEATHER.period_avg_wind_speed AS AVG(WEATHER.avg_wind_speed_fact)\n            WITH SYNONYMS = ('平均風速', '風速平均')\n            COMMENT = '期間内の全国平均風速（km/h）'\n    )\n\n    COMMENT = '携帯電話通信会社のネットワーク品質・設備・気象分析のためのセマンティックビュー。関東・近畿・東海・九州の50エリアの通信品質データを含む。2022年〜の日次通信品質データ（下り/上り速度、遅延、接続成功率、切断率、パケットロス率、トラフィック量）、2026-02-24時点の設備キャパシティスナップショット（基地局数、最大容量、負荷率）、および気象庁の全国日次平均気象データ（気温・降水量・湿度・日照時間・降雪量等）を統合。計測日と気象観測日がリレーションで紐づき、天候と通信品質の相関分析が可能。';"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000019",
   "metadata": {
    "name": "cell19"
   },
   "source": [
    "### セマンティックビューの確認\n",
    "\n",
    "正しく定義できたか確認しましょう。テーブル・ファクト・ディメンション・メトリクスの一覧が表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000020",
   "metadata": {
    "name": "cell20",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "DESCRIBE SEMANTIC VIEW TELECOM_ANALYST;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000021",
   "metadata": {
    "name": "cell21"
   },
   "source": [
    "---\n## Step 6: AI エージェントを組み立てる（Cortex Agent）\n\nいよいよエージェントを組み立てます。  \nここまで作ってきた **Cortex Search** と **Cortex Analyst** を組み合わせて、  \n1つの **AI エージェント** にまとめます。\n\n### エージェントの仕組み\n\n```\nユーザーの質問\n     │\n     ▼\n┌─────────────────────────┐\n│   Telecom Agent        │\n│  （オーケストレーター）   │\n│                        │\n│  質問の意図を判断して      │\n│  適切なツールを選択        │\n└────┬──────────┬─────────┘\n     │          │\n     ▼          ▼\n┌─────────┐ ┌──────────────┐\n│ Cortex  │ │   Cortex     │\n│ Search  │ │  Analyst     │\n│         │ │              │\n│エリア情報│  │通信品質・設備・│\n│の意味検索│ │気象の数値分析  │\n└─────────┘ └──────────────┘\n```\n\n### ツールの使い分け\n\n| 質問の種類 | 使われるツール | 例 |\n|-----------|---------------|----|\n| エリアの特徴・情報を知りたい | **Cortex Search** | 「関東のエリア一覧を教えて」 |\n| 数値の集計・分析をしたい | **Cortex Analyst** | 「2025年の月別通信速度は？」 |\n| 両方が必要 | **Search → Analyst** | 「逼迫エリアのトラフィック推移は？」 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000022",
   "metadata": {
    "name": "cell22",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE AGENT TELECOM_AI_HANDSON.ANALYTICS.TELECOM_AGENT\n",
    "    COMMENT = '携帯電話通信会社のネットワーク品質・設備・気象分析とエリア検索を行うエージェント'\n",
    "FROM SPECIFICATION $$\n",
    "{\n",
    "  \"models\": {\n",
    "    \"orchestration\": \"\"\n",
    "  },\n",
    "  \"instructions\": {\n",
    "    \"response\": \"あなたは携帯電話通信会社のネットワーク品質アナリストです。ユーザーからの質問に対して、通信品質データ（下り/上り速度、遅延、接続成功率、切断率、パケットロス率、トラフィック量）、設備キャパシティデータ（基地局数、最大容量、負荷率）、気象データを活用して回答してください。数値データは適切にフォーマットして表示してください（例：速度はMbps表記、トラフィックはGB表記、比率は%表記）。エリア名は正確に記載してください。分析結果には根拠となるデータを含めてください。日本語で丁寧に回答してください。可能であれば視覚化を提供してください。時系列のトレンドは線グラフをデフォルトとし、エリア比較は棒グラフを使用してください。天候と通信品質の関係（雨天時の品質低下、気温とトラフィック量の相関など）にも注意して分析してください。\",\n",
    "    \"orchestration\": \"エリアの詳細情報（エリア名、地方、エリアタイプ、AI説明文など）を検索する場合はcortex searchを使用し、通信品質・設備・気象の数値分析にはcortex analystを使用してください。複合的な質問の場合は、まずcortex searchでエリアを特定し、その結果をcortex analystに渡して分析してください。気温や天候と通信品質の関係を聞かれた場合は、cortex analystでネットワーク品質データと気象データを組み合わせて分析してください。\",\n",
    "    \"sample_questions\": [\n",
    "      {\n",
    "        \"question\": \"2025年の東京都心エリアの下り通信速度推移を月別で可視化してください。気温との相関も見たいです。\"\n",
    "      },\n",
    "      {\n",
    "        \"question\": \"トラフィック量が最も多い月はいつですか？どのエリアが特にトラフィックが集中しますか？\"\n",
    "      },\n",
    "      {\n",
    "        \"question\": \"現在設備が逼迫しているエリアを教えてください。それぞれのエリアの過去のトラフィック推移も見せてください。\"\n",
    "      },\n",
    "      {\n",
    "        \"question\": \"関東地方のエリア一覧とそれぞれの特徴を教えてください。\"\n",
    "      },\n",
    "      {\n",
    "        \"question\": \"2024年と2025年でトラフィックが大きく増加したエリアトップ5を教えてください。\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"tool_spec\": {\n",
    "        \"type\": \"cortex_analyst_text_to_sql\",\n",
    "        \"name\": \"TELECOM_ANALYST\",\n",
    "        \"description\": \"携帯電話通信会社のネットワーク品質データ、設備キャパシティデータ、全国日次気象データをクエリして分析します。下り/上り通信速度、遅延、接続成功率、切断率、パケットロス率、トラフィック量の集計や、気温・降水量と通信品質の相関分析に使用します。2022年〜2026年2月の日次通信品質データ、2026年2月時点の設備キャパシティスナップショット、2015年〜の全国平均気象データを含みます。\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"tool_spec\": {\n",
    "        \"type\": \"cortex_search\",\n",
    "        \"name\": \"AREA_SEARCH\",\n",
    "        \"description\": \"携帯電話通信エリアのマスタからエリア情報を検索します。エリア名、地方名、AIが分類したエリアタイプ（都心ビジネス、商業・繁華街、住宅地、観光・ウォーターフロント、郊外・地方）、AIが生成したエリア特性説明文などのテキスト検索に使用します。\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"tool_resources\": {\n",
    "    \"TELECOM_ANALYST\": {\n",
    "      \"semantic_view\": \"TELECOM_AI_HANDSON.ANALYTICS.TELECOM_ANALYST\"\n",
    "    },\n",
    "    \"AREA_SEARCH\": {\n",
    "      \"name\": \"TELECOM_AI_HANDSON.ANALYTICS.AREA_SEARCH_SERVICE\",\n",
    "      \"max_results\": 10\n",
    "    }\n",
    "  }\n",
    "}\n",
    "$$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000023",
   "metadata": {
    "name": "cell23"
   },
   "source": [
    "### エージェントの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000024",
   "metadata": {
    "name": "cell24",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "SHOW AGENTS IN SCHEMA TELECOM_AI_HANDSON.ANALYTICS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000025",
   "metadata": {
    "name": "cell25"
   },
   "source": [
    "---\n## Step 7: 動かしてみよう！\n\nエージェントが完成しました！Snowsight の **Cortex Agent UI** から以下のような質問を試してみましょう。\n\n### 試してみたい質問例\n\n**通信品質分析**\n- 「2025年の東京都心エリアの下り通信速度推移を月別で可視化してください。気温との相関も見たいです。」\n- 「2024年と2025年でトラフィックが大きく増加したエリアトップ5を教えてください。」\n\n**天候×通信品質**\n- 「降水量が多い日と少ない日で通信速度にどんな違いがありますか？」\n- 「台風シーズン（8-9月）の通信品質への影響を分析してください。」\n\n**設備管理**\n- 「現在設備が逼迫しているエリアを教えてください。それぞれの過去のトラフィック推移も見せてください。」\n- 「設備負荷率が注意レベルのエリアはどこですか？」\n\n**エリア検索**\n- 「関東地方のエリア一覧とそれぞれの特徴を教えてください。」\n- 「観光地エリアの通信品質の傾向を教えてください。」\n\n**Web検索**\n- 「2026年に予定されている5G展開計画について教えてください。」\n- 「最近の通信障害ニュースはありますか？」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dddc9e-1d48-471c-aa8b-cc1ad0edf888",
   "metadata": {
    "name": "cell48"
   },
   "source": [
    "---\n## Step 8【応用編】: カスタムモデルで予兆検知（Snowpark ML / Container Runtime）\n\n> **前提条件**: この Step は Snowflake Notebook の **Container Runtime** 上で実行します。  \n> `setup_database.sql` の STEP 12 でコンピュートプールの作成と Notebook への割り当てが完了しています。  \n> Snowsight の Notebook 設定で **Container Runtime が有効** になっていることを確認してください。  \n> scikit-learn / XGBoost は Container Runtime に **プリインストール済み** のため、追加インストールは不要です。\n\n### Step 3 との違い\n\n| | Step 3（ML Functions） | Step 8（Snowpark ML） |\n|---|---|---|\n| **言語** | SQL のみ | Python（OSS ライブラリ） |\n| **アルゴリズム** | Snowflake 組み込み（GBM / ARIMA） | 自由に選択（IsolationForest, XGBoost 等） |\n| **カスタマイズ性** | 外因変数・感度パラメータ | ハイパーパラメータ・特徴量エンジニアリング全般 |\n| **実行環境** | ウェアハウス | Container Runtime（コンピュートプール） |\n| **モデル管理** | SQL オブジェクト | **Model Registry**（バージョン管理・メタデータ） |\n\n### この Step でやること\n\n1. **IsolationForest** で切断率の異常検知（教師なし）\n2. **XGBRegressor** でトラフィック量の予測（教師あり）\n3. 両モデルを **Model Registry** に登録し、推論を実行"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "f5ff8549-1b29-4262-b875-da50a6854fd1",
   "metadata": {
    "name": "cell49",
    "language": "python"
   },
   "source": [
    "# Container Runtime にプリインストール済みのパッケージを確認\n",
    "# scikit-learn と xgboost が含まれていることを確認します\n",
    "!pip freeze | grep -E \"scikit-learn|xgboost\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "624ff4db-783c-4fb8-9fe8-472109dc7d47",
   "metadata": {
    "name": "cell50",
    "language": "python"
   },
   "source": "# ============================================================\n# データ取得: 通信品質 + 気象データを Pandas に読み込む\n# ============================================================\nimport pandas as pd\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n\n# 全国平均の日次通信品質 + 気象データを結合して取得\ndf = session.sql(\"\"\"\n    SELECT\n        nq.MEASURE_DATE,\n        AVG(nq.CALL_DROP_RATE)          AS CALL_DROP_RATE,\n        AVG(nq.TOTAL_TRAFFIC_GB)        AS TOTAL_TRAFFIC_GB,\n        AVG(nq.AVG_DOWNLOAD_MBPS)       AS AVG_DOWNLOAD_MBPS,\n        AVG(nq.AVG_LATENCY_MS)          AS AVG_LATENCY_MS,\n        AVG(nq.PACKET_LOSS_RATE)        AS PACKET_LOSS_RATE,\n        AVG(w.AVG_TEMPERATURE)          AS AVG_TEMPERATURE_C,\n        AVG(w.AVG_RAINFALL)             AS PRECIPITATION_MM,\n        AVG(w.AVG_WIND_SPEED)           AS WIND_SPEED_KMH\n    FROM TELECOM_AI_HANDSON.ANALYTICS.NETWORK_QUALITY nq\n    LEFT JOIN TELECOM_AI_HANDSON.ANALYTICS.DAILY_WEATHER w\n        ON nq.MEASURE_DATE = w.WEATHER_DATE\n    GROUP BY nq.MEASURE_DATE\n    ORDER BY nq.MEASURE_DATE\n\"\"\").to_pandas()\n\n# 日付型変換・欠損値除去\ndf[\"MEASURE_DATE\"] = pd.to_datetime(df[\"MEASURE_DATE\"])\ndf = df.dropna()\n\n# 訓練 / テスト分割（2025年以降をテスト）\ntrain = df[df[\"MEASURE_DATE\"] < \"2025-01-01\"].copy()\ntest  = df[df[\"MEASURE_DATE\"] >= \"2025-01-01\"].copy()\n\nprint(f\"訓練データ: {len(train)} 日, テストデータ: {len(test)} 日\")\nprint(f\"期間: {train['MEASURE_DATE'].min().date()} ~ {test['MEASURE_DATE'].max().date()}\")\ntrain.head()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "83ccd37d-b4d6-45be-b5cb-122c68a31f16",
   "metadata": {
    "name": "cell51"
   },
   "source": [
    "### パート A: IsolationForest で異常検知\n\n**IsolationForest** は、教師なし学習の異常検知アルゴリズムです。  \nデータポイントを「孤立させるのに必要な分割回数」で異常度を測定します。  \n異常なデータは少ない分割で孤立するため、**異常スコアが高く** なります。\n\n#### Step 3 の ANOMALY_DETECTION との違い\n\n| | ML Functions（Step 3） | IsolationForest（Step 8） |\n|---|---|---|\n| アルゴリズム | GBM（Gradient Boosting） | ランダムフォレストベースの孤立検知 |\n| ハイパーパラメータ | `prediction_interval` のみ | `n_estimators`, `contamination`, `max_features` 等 |\n| 特徴量 | 対象カラム + 外因変数 | 自由に設計（ラグ特徴量・移動平均等も可） |\n| 出力 | IS_ANOMALY（True/False） | 異常スコア（連続値） + ラベル |"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "2356a70e-f208-4685-9cfa-ab5664d846ae",
   "metadata": {
    "name": "cell52",
    "language": "python"
   },
   "source": [
    "# ============================================================\n",
    "# IsolationForest: 訓練 → Model Registry に登録\n",
    "# ============================================================\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "# --- 特徴量 ---\n",
    "FEATURE_COLS = [\n",
    "    \"CALL_DROP_RATE\", \"AVG_DOWNLOAD_MBPS\", \"AVG_LATENCY_MS\",\n",
    "    \"PACKET_LOSS_RATE\", \"PRECIPITATION_MM\", \"WIND_SPEED_KMH\"\n",
    "]\n",
    "\n",
    "# --- モデル訓練 ---\n",
    "# contamination: 異常と見なすデータの割合（5%）\n",
    "# n_estimators: 決定木の本数（多いほど安定）\n",
    "iso_model = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.05,\n",
    "    max_features=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "iso_model.fit(train[FEATURE_COLS])\n",
    "\n",
    "# --- 訓練データでの異常検知結果を確認 ---\n",
    "train_scores = iso_model.decision_function(train[FEATURE_COLS])\n",
    "train_pred   = iso_model.predict(train[FEATURE_COLS])\n",
    "print(f\"訓練データの異常件数: {(train_pred == -1).sum()} / {len(train_pred)}\")\n",
    "\n",
    "# --- Model Registry に登録 ---\n",
    "# target_platforms=[\"WAREHOUSE\"] でウェアハウスからの推論を有効化\n",
    "reg = Registry(session=session, database_name=\"TELECOM_AI_HANDSON\", schema_name=\"ANALYTICS\")\n",
    "\n",
    "# 既存モデルがあれば削除して再登録（冪等性を確保）\n",
    "try:\n",
    "    reg.delete_model(\"NQ_ISOLATION_FOREST\")\n",
    "    print(\"既存モデル NQ_ISOLATION_FOREST を削除しました\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "iso_mv = reg.log_model(\n",
    "    model=iso_model,\n",
    "    model_name=\"NQ_ISOLATION_FOREST\",\n",
    "    version_name=\"v1\",\n",
    "    conda_dependencies=[\"scikit-learn\"],\n",
    "    comment=\"IsolationForest: 通信品質の異常検知モデル（切断率+気象特徴量）\",\n",
    "    sample_input_data=train[FEATURE_COLS],\n",
    "    target_platforms=[\"WAREHOUSE\"],\n",
    "    metrics={\n",
    "        \"n_estimators\": 200,\n",
    "        \"contamination\": 0.05,\n",
    "        \"train_anomaly_ratio\": float((train_pred == -1).mean())\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"✅ Model Registry に登録完了: NQ_ISOLATION_FOREST v1\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "003c6e15-ddb1-464b-b9b9-9de21d942385",
   "metadata": {
    "name": "cell53",
    "language": "python"
   },
   "source": [
    "# ============================================================\n",
    "# IsolationForest: テストデータで推論 → 結果を確認\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "\n",
    "# --- テストデータで推論（ローカルの iso_model を直接使用）---\n",
    "test_pred   = iso_model.predict(test[FEATURE_COLS])\n",
    "test_scores = iso_model.decision_function(test[FEATURE_COLS])\n",
    "\n",
    "test[\"ISO_ANOMALY\"]       = (test_pred == -1)\n",
    "test[\"ISO_ANOMALY_SCORE\"] = test_scores\n",
    "\n",
    "anomaly_days = test[test[\"ISO_ANOMALY\"]]\n",
    "print(f\"テストデータの異常検知結果: {len(anomaly_days)} 日 / {len(test)} 日 を異常と判定\")\n",
    "print(f\"異常日の切断率: 平均 {anomaly_days['CALL_DROP_RATE'].mean():.3f}, \"\n",
    "      f\"最大 {anomaly_days['CALL_DROP_RATE'].max():.3f}\")\n",
    "print(f\"正常日の切断率: 平均 {test[~test['ISO_ANOMALY']]['CALL_DROP_RATE'].mean():.3f}\")\n",
    "\n",
    "# --- Registry 経由の推論（mv.run はウェアハウスで実行）---\n",
    "print(\"\\n--- Registry 経由の推論（mv.run → Warehouse）---\")\n",
    "iso_mv_loaded = reg.get_model(\"NQ_ISOLATION_FOREST\").version(\"v1\")\n",
    "# Snowpark DataFrame に変換して渡す\n",
    "test_sp = session.create_dataframe(test[FEATURE_COLS].head(5))\n",
    "registry_pred = iso_mv_loaded.run(test_sp, function_name=\"predict\")\n",
    "registry_pred.show()\n",
    "\n",
    "# --- 異常日の上位10件を表示 ---\n",
    "anomaly_days[[\"MEASURE_DATE\", \"CALL_DROP_RATE\", \"PRECIPITATION_MM\",\n",
    "              \"WIND_SPEED_KMH\", \"ISO_ANOMALY_SCORE\"]].sort_values(\n",
    "    \"ISO_ANOMALY_SCORE\").head(10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d6ab4981-01d5-4399-b6d9-fba3ca67f6eb",
   "metadata": {
    "name": "cell54"
   },
   "source": [
    "### パート B: XGBRegressor でトラフィック予測\n",
    "\n",
    "**XGBRegressor**（XGBoost の回帰モデル）を使って、トラフィック量を予測します。  \n",
    "Step 3 の `FORECAST` は時系列専用（ARIMA ベース）でしたが、ここでは **特徴量エンジニアリング** を自分で設計し、  \n",
    "気象データ・曜日・月などの特徴量を組み合わせた勾配ブースティング回帰モデルを構築します。\n",
    "\n",
    "#### カスタマイズのポイント\n",
    "\n",
    "| カスタム要素 | 内容 |\n",
    "|---|---|\n",
    "| **ラグ特徴量** | 前日・3日前・7日前のトラフィック量 |\n",
    "| **移動平均** | 直近7日間の移動平均 |\n",
    "| **カレンダー特徴量** | 曜日（0-6）、月（1-12） |\n",
    "| **気象特徴量** | 気温・降水量・風速 |\n",
    "| **ハイパーパラメータ** | `n_estimators`, `max_depth`, `learning_rate` を調整 |"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "ed9d9b02-3845-41e3-9dba-6289c16d20ab",
   "metadata": {
    "name": "cell55",
    "language": "python",
    "codeCollapsed": false
   },
   "source": [
    "# ============================================================\n",
    "# XGBRegressor: 特徴量エンジニアリング → 訓練 → Registry 登録\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# --- 特徴量エンジニアリング ---\n",
    "def add_features(data):\n",
    "    \"\"\"ラグ特徴量・移動平均・カレンダー特徴量を追加\"\"\"\n",
    "    d = data.copy()\n",
    "    d = d.sort_values(\"MEASURE_DATE\")\n",
    "    # ラグ特徴量\n",
    "    d[\"TRAFFIC_LAG_1\"]  = d[\"TOTAL_TRAFFIC_GB\"].shift(1)\n",
    "    d[\"TRAFFIC_LAG_3\"]  = d[\"TOTAL_TRAFFIC_GB\"].shift(3)\n",
    "    d[\"TRAFFIC_LAG_7\"]  = d[\"TOTAL_TRAFFIC_GB\"].shift(7)\n",
    "    # 移動平均\n",
    "    d[\"TRAFFIC_MA_7\"]   = d[\"TOTAL_TRAFFIC_GB\"].rolling(7).mean()\n",
    "    # カレンダー特徴量\n",
    "    d[\"DAY_OF_WEEK\"]    = d[\"MEASURE_DATE\"].dt.dayofweek\n",
    "    d[\"MONTH\"]          = d[\"MEASURE_DATE\"].dt.month\n",
    "    return d.dropna()\n",
    "\n",
    "train_fe = add_features(train)\n",
    "# テストデータ: 訓練データの末尾と連結してラグ特徴量を計算した後に分離\n",
    "all_data_fe = add_features(pd.concat([train, test], ignore_index=True))\n",
    "test_fe = all_data_fe[all_data_fe[\"MEASURE_DATE\"] >= \"2025-01-01\"].copy()\n",
    "\n",
    "XGB_FEATURES = [\n",
    "    \"TRAFFIC_LAG_1\", \"TRAFFIC_LAG_3\", \"TRAFFIC_LAG_7\", \"TRAFFIC_MA_7\",\n",
    "    \"DAY_OF_WEEK\", \"MONTH\",\n",
    "    \"AVG_TEMPERATURE_C\", \"PRECIPITATION_MM\", \"WIND_SPEED_KMH\"\n",
    "]\n",
    "TARGET = \"TOTAL_TRAFFIC_GB\"\n",
    "\n",
    "# --- モデル訓練 ---\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(\n",
    "    train_fe[XGB_FEATURES], train_fe[TARGET],\n",
    "    eval_set=[(test_fe[XGB_FEATURES], test_fe[TARGET])],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# --- 評価 ---\n",
    "test_fe[\"XGB_PREDICTED\"] = xgb_model.predict(test_fe[XGB_FEATURES])\n",
    "mae  = mean_absolute_error(test_fe[TARGET], test_fe[\"XGB_PREDICTED\"])\n",
    "rmse = np.sqrt(mean_squared_error(test_fe[TARGET], test_fe[\"XGB_PREDICTED\"]))\n",
    "print(f\"テストデータ評価: MAE = {mae:.2f} GB, RMSE = {rmse:.2f} GB\")\n",
    "\n",
    "# --- 特徴量の重要度 ---\n",
    "importance = pd.DataFrame({\n",
    "    \"FEATURE\": XGB_FEATURES,\n",
    "    \"IMPORTANCE\": xgb_model.feature_importances_\n",
    "}).sort_values(\"IMPORTANCE\", ascending=False)\n",
    "print(\"\\n特徴量の重要度:\")\n",
    "print(importance.to_string(index=False))\n",
    "\n",
    "# --- Model Registry に登録 ---\n",
    "# 既存モデルがあれば削除して再登録（冪等性を確保）\n",
    "try:\n",
    "    reg.delete_model(\"NQ_TRAFFIC_XGBOOST\")\n",
    "    print(\"既存モデル NQ_TRAFFIC_XGBOOST を削除しました\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# target_platforms=[\"WAREHOUSE\"] でウェアハウスからの推論を有効化\n",
    "xgb_mv = reg.log_model(\n",
    "    model=xgb_model,\n",
    "    model_name=\"NQ_TRAFFIC_XGBOOST\",\n",
    "    version_name=\"v1\",\n",
    "    conda_dependencies=[\"xgboost\"],\n",
    "    comment=\"XGBRegressor: トラフィック予測（ラグ+気象+カレンダー特徴量）\",\n",
    "    sample_input_data=train_fe[XGB_FEATURES],\n",
    "    target_platforms=[\"WAREHOUSE\"],\n",
    "    metrics={\"MAE\": float(mae), \"RMSE\": float(rmse)}\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Model Registry に登録完了: NQ_TRAFFIC_XGBOOST v1\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c29f1c9c-9cf6-4877-8e94-549b5ebd1f44",
   "metadata": {
    "name": "cell56",
    "language": "python"
   },
   "source": "# ============================================================\n# Model Registry の確認: 登録したモデル一覧\n# ============================================================\n\n# Registry に登録されたモデル一覧\nmodels = reg.show_models()\nprint(\"📦 登録済みモデル一覧:\")\nprint(models[[\"name\", \"comment\"]].to_string(index=False))\n\n# 各モデルのバージョンとメトリクスを確認\nfor model_name in [\"NQ_ISOLATION_FOREST\", \"NQ_TRAFFIC_XGBOOST\"]:\n    m = reg.get_model(model_name)\n    mv = m.version(\"v1\")\n    print(f\"\\n--- {model_name} v1 ---\")\n    print(f\"  メトリクス: {mv.show_metrics()}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "95e38b41-1c2e-483a-a173-46db5d3bd215",
   "metadata": {
    "name": "cell57",
    "language": "python"
   },
   "source": [
    "# ============================================================\n",
    "# Registry 経由の推論デモ: 直近30日の異常検知\n",
    "# ============================================================\n",
    "\n",
    "# 直近30日のデータを取得\n",
    "recent_df = session.sql(\"\"\"\n",
    "    SELECT\n",
    "        nq.MEASURE_DATE,\n",
    "        AVG(nq.CALL_DROP_RATE)          AS CALL_DROP_RATE,\n",
    "        AVG(nq.AVG_DOWNLOAD_MBPS)       AS AVG_DOWNLOAD_MBPS,\n",
    "        AVG(nq.AVG_LATENCY_MS)          AS AVG_LATENCY_MS,\n",
    "        AVG(nq.PACKET_LOSS_RATE)        AS PACKET_LOSS_RATE,\n",
    "        AVG(w.AVG_RAINFALL)             AS PRECIPITATION_MM,\n",
    "        AVG(w.AVG_WIND_SPEED)           AS WIND_SPEED_KMH\n",
    "    FROM TELECOM_AI_HANDSON.ANALYTICS.NETWORK_QUALITY nq\n",
    "    LEFT JOIN TELECOM_AI_HANDSON.ANALYTICS.DAILY_WEATHER w\n",
    "        ON nq.MEASURE_DATE = w.WEATHER_DATE\n",
    "    WHERE nq.MEASURE_DATE >= DATEADD('day', -30, CURRENT_DATE())\n",
    "    GROUP BY nq.MEASURE_DATE\n",
    "    ORDER BY nq.MEASURE_DATE DESC\n",
    "\"\"\").to_pandas()\n",
    "recent_df = recent_df.dropna()\n",
    "\n",
    "# --- ローカルの iso_model で推論 ---\n",
    "recent_pred   = iso_model.predict(recent_df[FEATURE_COLS])\n",
    "recent_scores = iso_model.decision_function(recent_df[FEATURE_COLS])\n",
    "\n",
    "recent_df[\"IS_ANOMALY\"]     = (recent_pred == -1)\n",
    "recent_df[\"ANOMALY_SCORE\"]  = recent_scores\n",
    "\n",
    "anomaly_count = recent_df[\"IS_ANOMALY\"].sum()\n",
    "print(f\"直近30日の異常検知結果: {anomaly_count} 日を異常と判定\")\n",
    "\n",
    "# --- Registry 経由の推論（mv.run → ウェアハウス）---\n",
    "# Snowpark DataFrame のカラム名をモデル登録時と一致させる\n",
    "print(\"\\n--- Registry 経由の推論（上位5件）---\")\n",
    "iso_model_ref = reg.get_model(\"NQ_ISOLATION_FOREST\").version(\"v1\")\n",
    "sample_sp = session.create_dataframe(recent_df[FEATURE_COLS].head(5).reset_index(drop=True))\n",
    "# create_dataframe は Pandas のカラム名をそのまま使うが、大文字に変換されるため\n",
    "# モデル登録時の sample_input_data と一致していれば動作する\n",
    "registry_result = iso_model_ref.run(sample_sp, function_name=\"predict\")\n",
    "registry_result.show()\n",
    "\n",
    "# --- 結果表示 ---\n",
    "recent_df[[\"MEASURE_DATE\", \"CALL_DROP_RATE\", \"PRECIPITATION_MM\",\n",
    "           \"WIND_SPEED_KMH\", \"ANOMALY_SCORE\", \"IS_ANOMALY\"]]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7594cdca-dd54-4d31-9575-f01c76ca4f5b",
   "metadata": {
    "name": "cell58"
   },
   "source": [
    "### Step 8 まとめ\n\nOSS ライブラリと Snowflake の **Container Runtime** + **Model Registry** を使って、カスタム ML パイプラインを構築しました。\n\n| モデル | ライブラリ | 用途 | カスタム要素 |\n|--------|-----------|------|------------|\n| **IsolationForest** | scikit-learn | 異常検知（教師なし） | `contamination` で異常割合を制御、多次元特徴量で検知 |\n| **XGBRegressor** | XGBoost | トラフィック予測 | ラグ特徴量・移動平均・カレンダー・気象の特徴量エンジニアリング |\n\n#### ML 開発サイクルの全体像\n\n```\nデータ取得（Snowpark session.sql → Pandas）\n     │\n     ▼\n特徴量エンジニアリング（Pandas）\n     │\n     ▼\nモデル訓練（scikit-learn / XGBoost）\n     │\n     ▼\nModel Registry に登録（reg.log_model + target_platforms=[\"WAREHOUSE\"]）\n     │\n     ▼\n推論（mv.run で Snowpark DataFrame を渡す → ウェアハウスで実行）\n     │\n     ▼\nTask + Alert で自動化（発展）\n```\n\n> **ポイント**: `target_platforms=[\"WAREHOUSE\"]` を指定して登録すると、ウェアハウス上で `mv.run()` による推論が実行できます。  \n> **発展**: Model Registry にはバージョン管理機能があるため、モデルを改善したら `v2`, `v3` と登録し、  \n> `DEFAULT_VERSION` を切り替えることで **ダウンタイムなし** でモデルを更新できます。"
   ],
   "attachments": {}
  }
 ]
}